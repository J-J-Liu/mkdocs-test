# WCS

这篇文章的结构相对完整且条理清晰，典型的技术论文结构如下：

**摘要（Abstract）**：

- 概括性介绍了论文的研究背景、研究问题和关键发现。主要讨论了如何分析和优化仓库级计算机（WSC）中的微架构性能问题。通过对超过2万台Google机器进行为期三年的数据分析，文章揭示了多种优化机会。
1. **引言（Introduction）**：
    - 提出了当前计算发展趋势，如云计算和仓库级计算的兴起，并强调了理解服务器应用与底层微架构互动的重要性。指出了之前研究的局限性，并说明本文的创新之处是对实时仓库级计算机的性能进行详细剖析。
2. **背景与方法论（Background and Methodology）**：
    
    描述了现代仓库级计算机的典型软件环境，并详细说明了论文使用的性能数据收集和分析方法，包括如何在Google数据中心中通过Google-Wide-Profiling（GWP）进行大规模的性能数据采集。
    
    - **WSC软件环境背景介绍（Background: WSC Software Deployment）**：
    
    **WSC软件API窄，服务间通信采用RPC方式，常用静态链接，使得指令占用空间大，工作负载更有多样性**
    
    - 本小节概述了典型的仓库级计算机软件环境。虽然具体案例基于Google的数据中心经验，但这些技术也适用于其他大型分布式系统或云平台。
    - 介绍了WSC的分布式、多层服务架构，其中每个服务通过远程过程调用（RPC）进行通信，数据传输通过协议缓冲区（Protocol Buffers）进行序列化和反序列化。
    - 还介绍了WSC系统中的小型服务和窄API，快速发布周期（每天或每周发布），以及如何通过共享代码库进行高效管理。
    - **持续性能分析（Continuous Profiling）**：
    
    **文章采用GWP实时地从服务器里偷数据，本节介绍它怎么偷的**
    
    - 本节介绍了Google使用的持续性能分析工具——Google-Wide Profiling (GWP)。
    - 该工具的工作原理是对数据中心中的部分机器进行随机采样，在短时间内收集性能数据（如通过Linux的`perf`工具），然后将数据汇总到Dremel数据库中以便进行分析。
    - 这种长期的数据采集为分析WSC中的微架构事件提供了基础，覆盖了数千台机器和数年时间。
    - **基于架构的性能数据收集（Architecture-Specific Collection）**：
    
    **本节介绍偷的数据有哪些限制：基于Intel Ivy Bridge处理器的20,000台机器、元数据、microbenchmark进行验证、避免时分复用以及要标准化**
    
    - 该部分解释了如何通过特定的处理器性能计数器来收集微架构相关的数据。研究基于Intel Ivy Bridge处理器的20,000台机器，按线程测量了每秒的性能计数器样本。
    - 数据收集的复杂性在于性能计数器的准确性（如避免计数器表达式的错误）和如何合理归因于相应的工作负载和二进制文件。
    - **性能计数器分析方法（Performance Counter Analysis）**：
    
    **嗯就是topdown**
    
    - 本节详细介绍了所使用的性能分析方法——Top-Down方法。该方法通过将处理器的微操作队列作为分界点，划分流水线槽中的操作为四个类别：Retiring（有用工作）、Front-End Bound（前端瓶颈）、Bad Speculation（错误推测）和Back-End Bound（后端瓶颈）。
    - 这种方法允许从顶层逐步深入地分析微架构的性能瓶颈，便于对WSC应用进行更细致的性能评估。
    - **工作负载选择（Workloads Selection）**：
    
    **精心烹饪的12个负载**
    
    - 论文选择了12个代表性负载进行深入分析，涵盖了不同的应用类型（如视频处理、搜索、存储服务等）以及不同的微架构行为（如数据缓存压力、前端瓶颈等）。
    - 这些工作负载的选择旨在展示WSC中工作负载的多样性，同时也进行大范围的平均值分析，以获得更加广泛的性能评估结果。
3. **工作负载的多样性（Workload Diversity）**：
    - 讨论了WSC中应用程序的多样性，并通过长期研究展示了在Google数据中心中不同应用程序的多样性和随时间变化的趋势。
4. **数据中心的"税收"（Datacenter Tax）**：
    - 定义并分析了所谓的数据中心"税收"（datacenter tax），即在不同应用程序之间共通的性能瓶颈，如协议缓冲区管理、RPC调用、压缩、内存分配等。这些都被认为是未来数据中心SoC优化的潜在方向。
    
    数据中心系统中一些常见的、耗费大量计算资源的底层功能模块，并将这些模块统称为“数据中心税”。这些功能虽然在每个应用中看似微小，但由于它们广泛存在于不同的应用中，累积起来对性能产生了较大的影响。
    
    主要的“数据中心税”组件包括以下几类：
    
    1. **协议缓冲区管理（Protocol Buffer Management）**：协议缓冲区是Google在内部使用的数据序列化格式，广泛应用于数据存储和传输中。序列化和反序列化协议缓冲区的数据是非常频繁的操作，消耗了大量的计算资源。作者建议可以通过硬件加速这一过程，类似于XML解析器的硬件加速。
    2. **远程过程调用（Remote Procedure Calls, RPCs）**：RPC在分布式系统中无处不在，其主要功能是处理负载平衡、加密和故障检测等任务。其一部分消耗来自于数据的简单传输。对于数据传输部分，作者提出可以通过通用的数据传输加速器来优化。
    3. **数据移动（Data Movement）**：数据的移动不仅限于RPC，还包括各种memcpy()和memmove()函数的调用，这些显式的数据移动占了大约4-5%的数据中心总计算周期。作者建议采用基于DRAM的优化技术来减少这类操作的开销。
    4. **压缩（Compression）**：大约四分之一的数据中心税周期花费在数据压缩和解压缩操作上。由于存在不同的压缩算法，作者认为通过硬件加速可以显著提高压缩/解压缩的效率。
    5. **内存分配（Memory Allocation）**：内存的分配和释放是数据中心中另一项重要的开销，尽管已经有许多软件优化，但硬件加速内存分配依然是一个值得探索的方向。
    6. **哈希算法（Hashing）**：哈希算法虽然占用的计算周期较小，但由于在各种服务器操作中被频繁使用，作者认为可以通过硬件加速来进一步提升效率。
    
    ### 结论：
    
    作者指出，尽管数据中心工作负载非常多样化，但通过分析得出，这些“数据中心税”组件在不同应用中普遍存在，且消耗了约22%-27%的总执行周期。通过针对这些组件进行硬件加速，能够显著提高数据中心的整体性能。
    
    这部分内容的核心是，作者认为数据中心中存在许多跨应用的公共低层功能模块，它们成为了性能的主要瓶颈，通过硬件加速这些关键模块是未来优化数据中心系统的有效途径。
    
5. **微架构分析（Microarchitecture Analysis）**：
    - 提出了基于"Top-Down"性能分析方法的微架构瓶颈分类，并详细分析了WSC的前端与后端瓶颈，如指令缓存问题和后端延迟。
    1. **SPEC基准测试的参考点**：
        - **400.perlbench**：这个基准测试代表了高指令执行率（IPC）并具有最大的指令缓存工作集（i-cache working set）。
        - **445.gobmk**：这个测试具有难以预测的分支，并且有很高的指令缓存未命中率（MPKI，Misses Per Kilo Instructions）。
        - **429.mcf和471.omnetpp**：这两个测试是**内存密集型**的，主要受内存延迟影响。
        - **433.milc**：也是内存密集型的，但主要关注的是**内存带宽**的限制。
    2. **数据中心工作负载的主要观察结果**：
        - **指令执行（Retiring uops）的比例很低**：相比SPEC基准测试中的429.mcf，数据中心工作负载的执行指令比例更低，这意味着大部分时间核心是在等待而不是执行指令。也就是说，许多时间被用来处理性能瓶颈，而不是实际执行有用的工作。
        - **后端压力大**：大多数瓶颈来自处理器的后端，除了search2和search3这两个应用程序外，超过60%的指令执行槽位（uop slots）因为后端的压力被阻塞。
        - **错误预测（Bad speculation）**：分支预测错误率的范围广泛，部分工作负载的错误率是445.gobmk和473.astar的0.5倍到2倍，而SPEC基准测试中的其他测试错误率则低于这个范围。
        - **前端压力**：与SPEC基准测试相比，数据中心工作负载中一个显著的区别是前端压力导致的大量执行阻塞，表明指令缓存不足的问题非常严重。下一节将更详细地探讨前端的这些问题。
6. **指令缓存瓶颈（Instruction Cache Bottlenecks）**：
    - 进一步探讨了WSC应用程序中由于指令缓存失效而导致的前端瓶颈，分析了指令工作集随时间的增长，并提出了可能的优化方法。
    
    这一部分讨论了**数据中心工作负载（WSC应用）**在前端指令缓存中的性能瓶颈，以及解决这些瓶颈的可能方法。
    
    ### 主要内容：
    
    1. **前端执行阻塞**：
        - WSC应用在前端存在大量的执行阻塞（15-30%的执行槽位被浪费），比传统的SPEC基准测试中的阻塞率高出2-3倍。这些阻塞主要是由于前端无法及时提供足够的指令给后端执行，即使后端已经准备好接收指令。
        - 作者指出，前端阻塞的原因主要是由于**指令缓存问题**，特别是因为WSC应用程序的代码规模较大，导致缓存中有许多“温热的代码”（即使用频率不高的代码），这些代码占用了缓存空间，减少了高频代码的命中率。
    2. **指令缓存未命中**：
        - 通过对前端阻塞周期（前端没有提供指令给后端的周期）的测量，发现前端完全没有提供指令的周期超过了5%，即使存在深度（40条指令以上）的前端缓冲区也无法吸收这些指令获取延迟。
        - 作者推测，导致这种现象的主要原因是**指令未命中L2缓存**，引发长时间的延迟，这一假设通过数据得到了验证。WSC应用的L2缓存指令未命中率通常为5-20 MPKI（每千条指令的未命中数），远远高于SPEC CPU2006基准测试的最差情况，并且比CloudSuite中规模化工作负载的未命中率高出50%。
    3. **代码规模和指令缓存压力**：
        - WSC应用的代码体积非常大，二进制文件通常达到几百MB。由于没有显著的热点（频繁执行的代码片段），指令缓存必须处理大量的代码片段，这导致缓存的工作集非常大。L2缓存不仅要存储指令，还要处理数据流，进一步加剧了竞争。
    4. **优化方向**：
        - **增大指令缓存容量**：虽然可以通过增加指令缓存的容量来缓解问题，但需要在更大的缓存和更高的延迟之间做出权衡。
        - **更复杂的指令预取器**：预取器能够提前将指令从内存加载到缓存中，避免缓存未命中带来的延迟。之前的研究表明，复杂的预取器在私有一级缓存中表现较好。
        - **缓存分区**：将指令和数据的缓存分离，尤其是在L2缓存中，可以减轻指令与数据之间的竞争。比如SPARC M7处理器架构就采用了完全分离的L2指令缓存和数据缓存。
    
    ### 总结：
    
    这一部分强调了WSC应用在前端（特别是指令缓存）面临的严重性能瓶颈，主要原因是这些应用程序的代码规模大，且缺少明显的执行热点。作者提出了一些可能的解决方案，如增大缓存、使用更智能的预取器以及缓存分区。
    
    A problem in the making 之后的内容：
    
    引入了一个新的测试方法，然后得出一些测试的指令范围增长迅猛，其他一些比较稳定
    
7. **核心后端行为（Core Back-End Behavior）**：
    - 讨论了WSC应用程序在后端的性能表现，特别是与缓存延迟相关的问题，并提出了对未来多核设计的影响。
    
    这一段讨论了数据中心工作负载的性能瓶颈，特别是其后端（Back-end）的限制以及指令级并行性（ILP）的影响。
    
    首先，作者指出，尽管大型指令工作集的负面影响会继续增长，但通过**Top-Down**分析发现，数据中心工作负载的主要瓶颈还是集中在**处理器的后端**。由于前端和后端的停顿较多，平均每周期执行的指令数（IPC）非常少，几乎是**SPECint基准测试**的两倍低，接近那些以内存为瓶颈的测试程序（如429.mcf、471.omnetpp和433.milc）。这与已发表的数据中心工作负载的研究结果一致，导致研究者探索适用于数据中心的小核处理器。
    
    接着，作者解释了后端停顿的两个主要原因：
    
    1. **数据缓存请求的延迟**：这是处理器后端限制的主要因素，特别是在数据密集型的工作负载下。
    2. **指令级并行性（ILP）的不足**：即在某些周期中，处理器不能同时执行足够多的指令。
    
    图11显示了约50-60%的处理器周期都被用来等待缓存层次结构中的数据加载，或者由于存储缓冲区容量不足而停顿。这些延迟占到了超过80%的后端停顿槽位。
    
    尽管数据缓存延迟是主要问题，但并不是所有周期都花费在等待数据缓存上。通过图12测量的ILP分布可以看到，大约72%的执行周期中，每个周期执行的指令数很低（1到2条指令，基于6宽的Ivy Bridge核心）。然而，在28%的执行周期中，3个或更多功能单元是被充分利用的。
    
    **解释：** 这种行为可能是由于数据中心应用程序存在细粒度的依赖性缓存访问和间歇性突发计算所致。这些突发的计算可以依赖于缓存访问，或者是独立的并且可以提取出更多的指令级并行性（ILP）。作者进一步指出，这两种情况之间的区别，特别是突发计算是否处于执行的关键路径上，可能会影响小核处理器的性能。这部分需要通过专门的仿真研究来深入理解。
    
    ### 总结：
    
    这一段强调了数据中心工作负载中后端限制的复杂性，尤其是数据缓存延迟对性能的影响，以及如何在某些周期中提取指令级并行性。通过分析后，作者提出了未来研究的方向，即需要进一步模拟和研究不同核设计在这种复杂工作负载下的表现。
    
    这一段主要讨论了**内存带宽利用率**在数据中心工作负载中的表现，并提出了一些相关的设计权衡问题。
    
    1. **低内存带宽利用率的观察**：
        - 作者指出，通过对内存带宽的测量，得出的结果显示内存带宽利用率非常低。在图13中，累积直方图展示了大量机器的内存带宽利用率。数据显示，95%的机器内存带宽利用率不超过31%，而最高测量值为68%。此外，在最后的几个百分位数中，有较长的尾部（即少数机器的带宽利用率较高）。
        - 作者提出，部分低带宽利用率可能与**CPU利用率低**有关，但这并不能完全解释该现象。例如，Barroso等人的研究显示，数据中心集群的**CPU中位利用率在40%-70%之间**，而此处测量的**带宽中位利用率仅为10%**，表明即使CPU利用率较高，内存带宽的需求仍然较低。
    2. **低带宽需求的推论**：
        - 这种低带宽需求与其他研究（如CloudSuite和一些新兴的数据中心工作负载研究）结果一致，表明许多数据中心应用程序的**带宽需求远低于预期**。
        - 由此可以推断，对于当今的数据中心应用，**内存延迟**比带宽更重要。这是因为这些应用程序并不需要大量的内存带宽，而是更依赖于数据能否快速从内存中提取。
    3. **设计权衡**：
        - 在设计**仓库级服务器（WSC）**时，低内存带宽利用率可能意味着设计者可以在内存带宽和其他硬件资源之间进行权衡。例如，减少内存控制器的数量，从而释放更多的芯片面积，用于增加更多的处理核心或加速器。
    4. **基准测试与实际需求的差异**：
        - 文章还指出，内存带宽未被充分利用的现象与某些**基准测试**中的结果有所不同。比如，SPECrate基准测试通常通过在多个核心上运行多个副本，可能将某些基准测试的内存瓶颈从**延迟**转向**带宽**。这种情况可能导致架构师们为了优化带宽而偏离了实际工作负载的需求。
    
    ### 总结：
    
    文章通过对内存带宽利用率的分析，指出在实际的数据中心工作负载中，带宽需求远低于预期，延迟是更重要的性能瓶颈。因此，在设计数据中心服务器时，可以考虑减少带宽配置，将资源用于更有价值的领域（如增加核心或加速器）。此外，这种现象与某些基准测试结果不符，提示在评估服务器设计时，不能仅依赖带宽相关的基准测试，而应关注实际应用的需求。
    
8. **同时多线程（Simultaneous Multithreading, SMT）**：
    - 分析了同时多线程技术在WSC应用中的表现，展示了它对前端与后端效率的影响。
    
    这一段讨论了**同时多线程技术（SMT）**对数据中心工作负载的影响，以及其在Ivy Bridge架构处理器中的表现。
    
    ### 主要内容概述：
    
    1. **SMT的使用**：
        - 文章提到，在之前的分析中，没有考虑到启用了SMT的Ivy Bridge处理器的影响。虽然SMT已经启用，但先前的性能分解是基于每个超线程（hyperthread）来进行的，假设每个超线程都能使用整个处理器的执行宽度。
        - 一般来说，SMT在工作负载中表现最为高效的情形是，不同线程的性能瓶颈不同，这样多个线程可以互补。数据中心应用中，前端和后端都有效率低下的部分，因此这些应用通常能从SMT中获益。
    2. **无法直接测量SMT关闭的影响**：
        - 作者指出，无法关闭SMT进行大规模测试（因为会影响大量用户服务）。但是，可以通过比较每个超线程的性能计数器与整个核心的计数器来估算SMT的效率。
        - 当一个核心上运行多个线程时，每个线程的性能会下降（因为需要共享微架构单元和缓存），但核心的整体利用率会提高。
    3. **SMT对后端的影响**：
        - SMT启用后，后端的功能单元利用率有所提高。图14的数据显示，在统计每个核心时，使用了3个或更多的执行端口的周期占比从28%上升到34%。
        - 这种提升是符合预期的，因为SMT可以帮助更好地利用处理器资源。
    4. **SMT对前端的影响**：
        - SMT可能会增加指令缓存的压力，因为多个线程可能需要同时从缓存中提取指令，进一步加剧了指令缓存的瓶颈。但另一方面，SMT也可能通过让一个超线程在等待时，另一个超线程继续提取指令，从而吸收长延迟的取指气泡。
        - 实验结果显示，SMT确实改善了前端利用率。图14显示，与每个超线程的计数相比，整个核心的前端瓶颈周期减少了（中位数从22%降至16%），前端饥饿周期也从5%降至4%。
    5. **SMT的局限性**：
        - 尽管SMT能够提高前端和后端的性能，数据表明，启用了SMT之后，仍有75%的样本显示每周期执行的指令数（IPC）不超过1.2，而处理器理论上的最大IPC为4.0。这意味着仍存在严重的延迟瓶颈（如L3缓存或主内存中的指令和数据提取延迟）。
        - 这种情况表明，可能有增加更多线程的空间（即更宽的SMT），如在一些服务器芯片中所看到的那样。由于先前的分析表明**内存带宽利用率很低**，即使增加更多的线程，带宽也不太可能成为瓶颈。
    6. **未来研究方向**：
        - 这些结果表明，可能需要进一步研究**更宽的SMT**（更多的超线程）与潜在的性能提升之间的权衡，特别是在处理可能的资源容量瓶颈和功耗问题时。
    
    ### 总结：
    
    文章通过分析SMT对数据中心工作负载的影响，发现SMT能够提高后端的功能单元利用率并减少前端瓶颈。然而，SMT的收益仍然有限，尤其是在处理器存在较大的延迟瓶颈时。未来可能需要研究更宽的SMT设计，并权衡其在性能和功耗方面的成本。
    
9. **相关工作（Related Work）**：
    - 概述了相关领域的研究工作，包括不同数据中心架构的设计和优化研究，以及与本文相关的实验和数据集。
10. **结论（Conclusions）**：
    - 总结了研究结果，提出了WSC应用程序的主要特征，如低指令每周期（IPC）、大指令集、双峰型指令级并行性（ILP）等，并指出这些观察结果应如何影响未来的数据中心处理器设计。

文章的结构清晰，逐步展开了从研究背景、方法论到具体问题和优化方向的讨论，最终给出结论并展望未来的研究方向。